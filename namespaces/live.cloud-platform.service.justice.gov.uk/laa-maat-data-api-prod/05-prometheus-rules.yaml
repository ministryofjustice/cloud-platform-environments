apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: laa-maat-data-api-prod
  labels:
    role: alert-rules
  name: prometheus-custom-rules-laa-maat-data-api
spec:
  groups:
    - name: application-rules
      rules:
        - alert: Client Response Errors (excluding 400, 401, 404)
          expr: sum(increase(http_server_requests_seconds_count{namespace="laa-maat-data-api-prod",outcome="CLIENT_ERROR",status!~"4(00|01|04)"}[10m])) > 1
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: There has been an increase in client error responses (excluding 400, 401, 404) in the MAAT Data API running on production in the past 10 minutes. This may indicate a problem with clients calling the application - including intrusion attempts.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: Client Response Errors - 400 Bad request
          expr: sum(increase(http_server_requests_seconds_count{status="400",outcome="CLIENT_ERROR",namespace="laa-maat-data-api-prod"}[10m])) > 10
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: There has been an increase in Bad Request Errors in the MAAT Data API running on production in the past 10 minutes. This may indicate a problem with clients calling the application - including intrusion attempts.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: Client Response Errors - 401 Unauthorised
          expr: sum(increase(http_server_requests_seconds_count{status="401",outcome="CLIENT_ERROR",namespace="laa-maat-data-api-prod"}[10m])) > 1
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: There has been an increase in Unauthorised requests in the MAAT Data API running on production in the past 10 minutes. This may indicate a problem with clients calling the application - including intrusion attempts.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: Client Response Errors (404 Errors)
          expr: sum(increase(http_server_requests_seconds_count{status="404",outcome="CLIENT_ERROR",namespace="laa-maat-data-api-prod"}[10m])) > 1
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: There has been an increase in 404 error responses in the MAAT Data API running on production in the past 10 minutes. This may indicate a problem with clients calling the application - including intrusion attempts.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: MAAT Data API Rollback Error
          expr: sum(increase(http_server_requests_seconds_count{status="555", namespace="laa-maat-data-api-prod"}[10m])) > 10
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: There has been an increase in rollbacks from the MAAT Data API running on production in the past 10 minutes. This may indicate a problem with the server processing client requests - likely a bug.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: Server Response Error
          expr: sum(increase(http_server_requests_seconds_count{outcome="SERVER_ERROR", namespace="laa-maat-data-api-prod"}[10m])) > 1
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: There has been an increase in server error response from the MAAT Data API running on production in the past 10 minutes. This may indicate a problem with the server processing client requests - likely a bug.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: JVM CPU Usage
          expr: (process_cpu_usage{namespace="laa-maat-data-api-prod"} * 100) > 95
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: The MAAT Data API service running on a production pod has been using over 95% of the CPU for a minute. This may indicate the pods running this application require more CPU resources.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: System CPU Usage
          expr: (system_cpu_usage{namespace="laa-maat-data-api-prod"} * 100) > 95
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: A pod that runs the MAAT Data API service on production has been using over 95% of the pod's CPU for a minute. This may indicate there is some underlying process other than our application on the pod that is using up all the CPU resources and warrants further investigation.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: Object Heap Memory Usage - Tenured Gen
          expr: ((jvm_memory_used_bytes{area="heap", id="Tenured Gen", namespace="laa-maat-data-api-prod"}/jvm_memory_max_bytes{area="heap", id="Tenured Gen", namespace="laa-maat-data-api-prod"})*100) > 95
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: Over 95% of the "Tenured Gen" object heap memory on a production pod had been used. This may indicate our application needs more object heap memory or we have a memory resource leak in our application.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: Object Heap Memory Usage - Survivor Space
          expr: ((jvm_memory_used_bytes{area="heap", id="Survivor Space", namespace="laa-maat-data-api-prod"}/jvm_memory_max_bytes{area="heap", id="Survivor Space", namespace="laa-maat-data-api-prod"})*100) > 95
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: Over 95% of the "Survivor Space" object heap memory on a production pod had been used. This may indicate our application needs more object heap memory or we have a memory resource leak in our application.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: Object Heap Memory Usage - Eden Space
          expr: ((jvm_memory_used_bytes{area="heap", id="Eden Space", namespace="laa-maat-data-api-prod"}/jvm_memory_max_bytes{area="heap", id="Eden Space", namespace="laa-maat-data-api-prod"})*100) > 95
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: Over 95% of the "Eden Space" object heap memory on a production pod had been used. This may indicate our application needs more object heap memory or we have a memory resource leak in our application.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: Response Time Excessive
          expr: (sum(http_server_requests_seconds_sum{outcome="SUCCESS", namespace="laa-maat-data-api-prod"})/sum(http_server_requests_seconds_count{outcome="SUCCESS", namespace="laa-maat-data-api-prod"})) > 5
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: The response time for successful responses on production is taking on average over five seconds. This indicates responses to our clients is taking too long.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: Logging Error
          expr: sum(increase(logback_events_total{level="error", namespace="$namespace"}[10m])) > 1
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: There had been an error in the logs of the MAAT Data API in the past 10 minutes. This indicates that there may be a bug with the application.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/3c878b0d42914a8fa421f798cfb3ba8f/laa-maat-data-api?var-namespace=laa-maat-data-api-prod
        - alert: Instance-Down
          expr: absent(up{namespace="laa-maat-data-api-prod"}) == 1
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: The production instance of the MAAT Data API has been down for >1m.
        - alert: Quota-Exceeded
          expr: 100 * kube_resourcequota{job="kube-state-metrics",type="used",namespace="laa-maat-data-api-prod"} / ignoring(instance, job, type) (kube_resourcequota{job="kube-state-metrics",type="hard",namespace="laa-maat-data-api-prod"} > 0) > 90
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value}}% of its {{ $labels.resource }} quota.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubequotaexceeded
        - alert: KubePodCrashLooping
          expr: round(rate(kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace="laa-maat-data-api-prod"}[10m]) * 60 * 10) > 1
          for: 5m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting excessively
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
        - alert: Increase in 403 Blocked Requests
          expr: sum(increase(nginx_ingress_controller_requests{exported_namespace="laa-maat-data-api-prod", ingress="laa-maat-data-api", status="403"}[5m])) > 1
          for: 1m
          labels:
            severity: laa-maat-data-api-prod
            namespace: laa-maat-data-api-prod
          annotations:
            message: The rate of requests blocked by the internal ingress has been increasing over the past 5 minutes.
            dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/f1e13059dfd23fdcaf479f4fa833f92610c2dfa5/kubernetes-ingress-traffic?orgId=1&var-namespace=laa-maat-data-api-prod&var-ingress=laa-maat-data-api