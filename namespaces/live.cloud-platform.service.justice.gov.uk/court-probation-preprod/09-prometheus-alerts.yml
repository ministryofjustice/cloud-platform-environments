apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: prepare-a-case
    prometheus: cloud-platform
  name: pic-prometheus-k8s
  namespace: court-probation-preprod
spec:
  groups:
  - name:  court-hearing-event-receiver-app
    rules:
    - alert:  KubePodCrashLooping
      annotations:
        dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf "%.2f" $value }} times every 5 minutes.
        runbook_url: https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-pod-crashlooping
        Summary: Pod is crash looping.
      expr: rate(kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-hearing-event-receiver.*"}[10m]) * 60 * 5 > 0
      for:  15m
      labels:
        Severity:  slack-hmpps-user-preferences
    - alert: KubePodNotReady
      annotations:
        dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
        runbook_url: https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-pod-notready
        Summary: Pod has been in a non-ready state for more than 15 minutes.
      expr: sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-hearing-event-receiver.*", phase=~"Pending|Unknown"}) > 0
      for: 15m
      labels:
        Severity:  slack-hmpps-user-preferences
    - alert:       KubeDeploymentGenerationMismatch
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-deployment-generation-mismatch
        Summary:        Deployment generation mismatch due to possible roll-back
      expr:             (kube_deployment_status_observed_generation{job="kube-state-metrics", namespace=~"court-probation-preprod", deployment=~"court-hearing-event-receiver"} != kube_deployment_metadata_generation{job="kube-state-metrics", namespace=~"court-probation-preprod", deployment=~"court-hearing-event-receiver"})
      for:  15m
      labels:
        Severity:  slack-hmpps-user-preferences
    - alert:       KubeDeploymentReplicasMismatch
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer 15 minutes.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-deployment-replicas-mismatch
        Summary:        Deployment has not matched the expected number of replicas.
      expr:             (kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~"court-probation-preprod", deployment=~"court-hearing-event-receiver"} != kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~"court-probation-preprod", deployment=~"court-hearing-event-receiver"})
      for:  15m
      labels:
        Severity:  slack-hmpps-user-preferences
    - alert:       KubeContainerWaiting
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        Description:    Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container }} has been in waiting state for longer than 1 hour.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-container-waiting
        Summary:        Pod container waiting longer than 1 hour
      expr:             sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-hearing-event-receiver.*"}) > 0

      for:  1h
      labels:
        Severity:  slack-hmpps-user-preferences
    - alert:       KubeCronJobRunning
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-cronjob-running
        Summary:        CronJob taking a long time to complete.
      expr: time() - kube_cronjob_next_schedule_time{job="kube-state-metrics", namespace=~"court-probation-preprod", cronjob=~"court-hearing-event-receiver.*"} > 3600
      for: 1h
      labels:
        Severity:  slack-hmpps-user-preferences
      expr: label_replace(label_replace(max(kube_job_status_start_time{namespace=~"court-probation-preprod"} * on(job_name,namespace) GROUP_RIGHT() kube_job_owner{owner_name!="", namespace=~"court-probation-preprod"}) by (job_name, owner_name, namespace) == on(owner_name) group_left() max(kube_job_status_start_time{namespace=~"court-probation-preprod"} * on(job_name,namespace) GROUP_RIGHT() kube_job_owner{owner_name!="", namespace=~"court-probation-preprod"}) by (owner_name), "job", "$1", "job_name", "(.+)"), "cronjob", "$1", "owner_name", "(.+)")
      record: job:kube_job_status_start_time_court_probation_preprod:max
      expr: clamp_max(job:kube_job_status_start_time_court_probation_preprod:max,1) * on(job) group_left() label_replace(label_replace((kube_job_status_failed{namespace=~"court-probation-preprod"} != 0),"job", "$1", "job_name", "(.+)"),"cronjob", "$1", "owner_name", "(.+)")
      record:  job:kube_job_status_failed_court_probation_preprod:sum
    - alert:   CronJobStatusFailed
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is failing.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-cronjob-failed
        Summary:        CronJob is failing.
      expr:             (job:kube_job_status_failed_court_probation_preprod:sum
* on(cronjob,namespace) group_left()
(kube_cronjob_spec_suspend == 0))

      labels:
        Severity:  slack-hmpps-user-preferences
    - alert:       KubeHpaReplicasMismatch
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        Description:    HPA {{ $labels.namespace }}/{{ $labels.hpa }} has not matched the desired number of replicas for longer than 15 minutes.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-hpa-replicas-mismatch
        Summary:        HPA has not matched desired number of replicas.
      expr:             ((kube_hpa_status_desired_replicas{job="kube-state-metrics", namespace=~"court-probation-preprod", hpa=~"court-hearing-event-receiver"}
  !=
kube_hpa_status_current_replicas{job="kube-state-metrics"})
  and
(kube_hpa_status_current_replicas{job="kube-state-metrics"}
  >
kube_hpa_spec_min_replicas{job="kube-state-metrics"})
  and
(kube_hpa_status_current_replicas{job="kube-state-metrics"}
  <
kube_hpa_spec_max_replicas{job="kube-state-metrics"})
  and
changes(kube_hpa_status_current_replicas{job="kube-state-metrics"}[15m]) == 0)

      for:  15m
      labels:
        Severity:  slack-hmpps-user-preferences
    - alert:       KubeHpaMaxedOut
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        Description:    HPA {{ $labels.namespace }}/{{ $labels.hpa }} has been running at max replicas for longer than 15 minutes.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-hpa-maxed-out
        Summary:        HPA is running at max replicas
      expr:             (kube_hpa_status_current_replicas{job="kube-state-metrics", namespace=~"court-probation-preprod", hpa=~"court-hearing-event-receiver"}
  ==
kube_hpa_spec_max_replicas{job="kube-state-metrics"})

      for:  15m
      labels:
        Severity:  slack-hmpps-user-preferences
    - alert:       KubeQuotaExceeded
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value }}% of its {{ $labels.resource }} quota.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-quota-exceeded
        Summary:        Namespace quota has exceeded the limits.
      expr:             (100 * kube_resourcequota{namespace=~"court-probation-preprod", job="kube-state-metrics", type="used"}
  / ignoring(instance, job, type)
(kube_resourcequota{namespace=~"court-probation-preprod", job="kube-state-metrics", type="hard"} > 0)
  > 90)
      for:  15m
      labels:
        Severity:  slack-hmpps-user-preferences
    - alert:       KubeContainerOOMKilled
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOM Killed (out of memory) {{ $value }} times in the last 10 minutes.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-container-oom-killed
        Summary:        Kubernetes container OOM killed (instance {{ $labels.instance }})
      expr:             ((kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-hearing-event-receiver.*"}
 - kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-hearing-event-receiver.*"} offset 10m >= 1)
  and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-hearing-event-receiver.*", reason="OOMKilled"}[10m]) == 1)
      for:  0m
      labels:
        Severity:  slack-hmpps-user-preferences
Events:            <none>


Name:         court-hearing-event-receiver-ingress
Namespace:    court-probation-preprod
labels:       app.kubernetes.io/instance=court-hearing-event-receiver
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/name=generic-prometheus-alerts
              app.kubernetes.io/version=1.0.0
              helm.sh/chart=generic-prometheus-alerts-1.3.3
              prometheus=cloud-platform
annotations:  meta.helm.sh/release-name: court-hearing-event-receiver
              meta.helm.sh/release-namespace: court-probation-preprod
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2021-12-23T11:21:54Z
  Generation:          2
  Resource Version:    1390602685
  UID:                 13945918-afb8-4093-89e2-7af15278e212
Spec:
  Groups:
    Name:  court-hearing-event-receiver-ingress
    rules:
      alert:  5xxErrorResponses
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/golden-signals/golden-signals?orgId=1&var-namespace=court-probation-preprod&var-service=court-hearing-event-receiver
        message:        Ingress {{ $labels.exported_namespace }}/{{ $labels.ingress }} is serving 5xx responses.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#ingress-5xx-error-responses
      expr:             avg(rate(nginx_ingress_controller_requests{exported_namespace=~"court-probation-preprod", ingress=~"court-hearing-event-receiver.*", status=~"5.*"}[1m]) > 0) by (ingress, exported_namespace)
      for:              1m
      labels:
        Severity:  slack-hmpps-user-preferences
      alert:       RatelimitBlocking
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/golden-signals/golden-signals?orgId=1&var-namespace=court-probation-preprod&var-service=court-hearing-event-receiver
        message:        Rate limit is being applied on ingress {{ $labels.exported_namespace }}/{{ $labels.ingress }}.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#ingress-rate-limiting
      expr:             avg(rate(nginx_ingress_controller_requests{exported_namespace=~"court-probation-preprod", ingress=~"court-hearing-event-receiver.*", status="429"}[1m]) > 0) by (ingress, exported_namespace)
      for:              1m
      labels:
        Severity:  slack-hmpps-user-preferences
      alert:       ModSecurityBlocking
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/golden-signals/golden-signals?orgId=1&var-namespace=court-probation-preprod&var-service=court-hearing-event-receiver
        message:        Mod_Security is blocking ingress {{ $labels.exported_namespace }}/{{ $labels.ingress }}. Blocking http requests at rate of {{ printf "%.2f" $value }} per second.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#ingress-modsecurity-blocking
      expr:             avg(rate(nginx_ingress_controller_requests{exported_namespace=~"court-probation-preprod", ingress=~"court-hearing-event-receiver.*", status="406"}[1m]) > 0.33) by (ingress, exported_namespace)
      for:              1m
      labels:
        Severity:  slack-hmpps-user-preferences
Events:            <none>


Name:         court-list-splitter-app
Namespace:    court-probation-preprod
labels:       app.kubernetes.io/instance=court-list-splitter
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/name=generic-prometheus-alerts
              app.kubernetes.io/version=1.0.0
              helm.sh/chart=generic-prometheus-alerts-1.3.3
              prometheus=cloud-platform
annotations:  meta.helm.sh/release-name: court-list-splitter
              meta.helm.sh/release-namespace: court-probation-preprod
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2021-12-23T11:26:19Z
  Generation:          2
  Resource Version:    1390875811
  UID:                 15e93eb6-d7e2-44fc-a166-d62bec21f686
Spec:
  Groups:
    Name:  court-list-splitter-app
    rules:
      alert:  KubePodCrashLooping
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf "%.2f" $value }} times every 5 minutes.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-pod-crashlooping
        Summary:        Pod is crash looping.
      expr:             rate(kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-list-splitter.*"}[10m]) * 60 * 5 > 0

      for:  15m
      labels:
        Severity:  prepare-a-case
      alert:       KubePodNotReady
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-pod-notready
        Summary:        Pod has been in a non-ready state for more than 15 minutes.
      expr:             sum by (namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-list-splitter.*", phase=~"Pending|Unknown"}) > 0
      for:              15m
      labels:
        Severity:  prepare-a-case
      alert:       KubeDeploymentGenerationMismatch
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-deployment-generation-mismatch
        Summary:        Deployment generation mismatch due to possible roll-back
      expr:             (kube_deployment_status_observed_generation{job="kube-state-metrics", namespace=~"court-probation-preprod", deployment=~"court-list-splitter"}
  !=
kube_deployment_metadata_generation{job="kube-state-metrics", namespace=~"court-probation-preprod", deployment=~"court-list-splitter"})
      for:  15m
      labels:
        Severity:  prepare-a-case
      alert:       KubeDeploymentReplicasMismatch
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer 15 minutes.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-deployment-replicas-mismatch
        Summary:        Deployment has not matched the expected number of replicas.
      expr:             (kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~"court-probation-preprod", deployment=~"court-list-splitter"}
  !=
kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~"court-probation-preprod", deployment=~"court-list-splitter"})
      for:  15m
      labels:
        Severity:  prepare-a-case
      alert:       KubeContainerWaiting
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        Description:    Pod {{ $labels.namespace }}/{{ $labels.pod }} container {{ $labels.container }} has been in waiting state for longer than 1 hour.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-container-waiting
        Summary:        Pod container waiting longer than 1 hour
      expr:             sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-list-splitter.*"}) > 0

      for:  1h
      labels:
        Severity:  prepare-a-case
      alert:       KubeCronJobRunning
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-cronjob-running
        Summary:        CronJob taking a long time to complete.
      expr:             time() - kube_cronjob_next_schedule_time{job="kube-state-metrics", namespace=~"court-probation-preprod", cronjob=~"court-list-splitter.*"} > 3600
      for:              1h
      labels:
        Severity:  prepare-a-case
      expr:        label_replace(
  label_replace(
    max(
      kube_job_status_start_time{namespace=~"court-probation-preprod"}
      * on(job_name,namespace) GROUP_RIGHT()
      kube_job_owner{owner_name!="", namespace=~"court-probation-preprod"}
    )
    by (job_name, owner_name, namespace)
    == on(owner_name) group_left()
    max(
      kube_job_status_start_time{namespace=~"court-probation-preprod"}
      * on(job_name,namespace) GROUP_RIGHT()
      kube_job_owner{owner_name!="", namespace=~"court-probation-preprod"}
    )
    by (owner_name),
  "job", "$1", "job_name", "(.+)"),
"cronjob", "$1", "owner_name", "(.+)")

      record:  job:kube_job_status_start_time_court_probation_preprod:max
      expr:    clamp_max(job:kube_job_status_start_time_court_probation_preprod:max,1)
  * on(job) group_left()
  label_replace(
    label_replace(
      (kube_job_status_failed{namespace=~"court-probation-preprod"} != 0),
      "job", "$1", "job_name", "(.+)"),
    "cronjob", "$1", "owner_name", "(.+)")

      record:  job:kube_job_status_failed_court_probation_preprod:sum
      alert:   CronJobStatusFailed
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is failing.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-cronjob-failed
        Summary:        CronJob is failing.
      expr:             (job:kube_job_status_failed_court_probation_preprod:sum
* on(cronjob,namespace) group_left()
(kube_cronjob_spec_suspend == 0))

      labels:
        Severity:  prepare-a-case
      alert:       KubeHpaReplicasMismatch
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        Description:    HPA {{ $labels.namespace }}/{{ $labels.hpa }} has not matched the desired number of replicas for longer than 15 minutes.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-hpa-replicas-mismatch
        Summary:        HPA has not matched desired number of replicas.
      expr:             ((kube_hpa_status_desired_replicas{job="kube-state-metrics", namespace=~"court-probation-preprod", hpa=~"court-list-splitter"}
  !=
kube_hpa_status_current_replicas{job="kube-state-metrics"})
  and
(kube_hpa_status_current_replicas{job="kube-state-metrics"}
  >
kube_hpa_spec_min_replicas{job="kube-state-metrics"})
  and
(kube_hpa_status_current_replicas{job="kube-state-metrics"}
  <
kube_hpa_spec_max_replicas{job="kube-state-metrics"})
  and
changes(kube_hpa_status_current_replicas{job="kube-state-metrics"}[15m]) == 0)

      for:  15m
      labels:
        Severity:  prepare-a-case
      alert:       KubeHpaMaxedOut
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        Description:    HPA {{ $labels.namespace }}/{{ $labels.hpa }} has been running at max replicas for longer than 15 minutes.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-hpa-maxed-out
        Summary:        HPA is running at max replicas
      expr:             (kube_hpa_status_current_replicas{job="kube-state-metrics", namespace=~"court-probation-preprod", hpa=~"court-list-splitter"}
  ==
kube_hpa_spec_max_replicas{job="kube-state-metrics"})

      for:  15m
      labels:
        Severity:  prepare-a-case
      alert:       KubeQuotaExceeded
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value }}% of its {{ $labels.resource }} quota.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-quota-exceeded
        Summary:        Namespace quota has exceeded the limits.
      expr:             (100 * kube_resourcequota{namespace=~"court-probation-preprod", job="kube-state-metrics", type="used"}
  / ignoring(instance, job, type)
(kube_resourcequota{namespace=~"court-probation-preprod", job="kube-state-metrics", type="hard"} > 0)
  > 90)
      for:  15m
      labels:
        Severity:  prepare-a-case
      alert:       KubeContainerOOMKilled
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/application-alerts/application-alerts?orgId=1&var-namespace=court-probation-preprod
        message:        Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOM Killed (out of memory) {{ $value }} times in the last 10 minutes.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#application-container-oom-killed
        Summary:        Kubernetes container OOM killed (instance {{ $labels.instance }})
      expr:             ((kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-list-splitter.*"}
 - kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-list-splitter.*"} offset 10m >= 1)
  and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{job="kube-state-metrics", namespace=~"court-probation-preprod", pod=~"court-list-splitter.*", reason="OOMKilled"}[10m]) == 1)
      for:  0m
      labels:
        Severity:  prepare-a-case
Events:            <none>


Name:         court-list-splitter-ingress
Namespace:    court-probation-preprod
labels:       app.kubernetes.io/instance=court-list-splitter
              app.kubernetes.io/managed-by=Helm
              app.kubernetes.io/name=generic-prometheus-alerts
              app.kubernetes.io/version=1.0.0
              helm.sh/chart=generic-prometheus-alerts-1.3.3
              prometheus=cloud-platform
annotations:  meta.helm.sh/release-name: court-list-splitter
              meta.helm.sh/release-namespace: court-probation-preprod
API Version:  monitoring.coreos.com/v1
Kind:         PrometheusRule
Metadata:
  Creation Timestamp:  2021-12-23T11:26:19Z
  Generation:          2
  Resource Version:    1390875815
  UID:                 39aafcd5-a123-4cca-b14c-eb4fb01748dd
Spec:
  Groups:
    Name:  court-list-splitter-ingress
    rules:
      alert:  5xxErrorResponses
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/golden-signals/golden-signals?orgId=1&var-namespace=court-probation-preprod&var-service=court-list-splitter
        message:        Ingress {{ $labels.exported_namespace }}/{{ $labels.ingress }} is serving 5xx responses.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#ingress-5xx-error-responses
      expr:             avg(rate(nginx_ingress_controller_requests{exported_namespace=~"court-probation-preprod", ingress=~"court-list-splitter.*", status=~"5.*"}[1m]) > 0) by (ingress, exported_namespace)
      for:              1m
      labels:
        Severity:  prepare-a-case
      alert:       RatelimitBlocking
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/golden-signals/golden-signals?orgId=1&var-namespace=court-probation-preprod&var-service=court-list-splitter
        message:        Rate limit is being applied on ingress {{ $labels.exported_namespace }}/{{ $labels.ingress }}.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#ingress-rate-limiting
      expr:             avg(rate(nginx_ingress_controller_requests{exported_namespace=~"court-probation-preprod", ingress=~"court-list-splitter.*", status="429"}[1m]) > 0) by (ingress, exported_namespace)
      for:              1m
      labels:
        Severity:  prepare-a-case
      alert:       ModSecurityBlocking
      annotations:
        dashboard_url:  https://grafana.live.cloud-platform.service.justice.gov.uk/d/golden-signals/golden-signals?orgId=1&var-namespace=court-probation-preprod&var-service=court-list-splitter
        message:        Mod_Security is blocking ingress {{ $labels.exported_namespace }}/{{ $labels.ingress }}. Blocking http requests at rate of {{ printf "%.2f" $value }} per second.
        runbook_url:    https://github.com/ministryofjustice/hmpps-helm-charts/blob/main/charts/generic-prometheus-alerts/RUNBOOK.md#ingress-modsecurity-blocking
      expr:             avg(rate(nginx_ingress_controller_requests{exported_namespace=~"court-probation-preprod", ingress=~"court-list-splitter.*", status="406"}[1m]) > 0.33) by (ingress, exported_namespace)
      for:              1m
      labels:
        Severity:  prepare-a-case
Events:            <none>