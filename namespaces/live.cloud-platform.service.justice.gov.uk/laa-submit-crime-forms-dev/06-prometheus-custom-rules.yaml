apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  namespace: laa-submit-crime-forms-dev
  labels:
    prometheus: cloud-platform
    role: alert-rules
  name: prometheus-custom-rules-laa-submit-crime-forms-dev
spec:
  groups:
  - name: application-rules
    rules:
    - alert: NamespaceDown
      expr: absent(up{namespace="laa-submit-crime-forms-dev"}) == 1
      for: 1m
      labels:
        severity: laa-crime-forms-team-pre-prod
      annotations:
        message: Namespace {{ $labels.namespace }} is down.
        runbook_url: https://dsdmoj.atlassian.net/wiki/spaces/CRM457/pages/4634181917/Runbooks
        dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/laa-crime-store-grafana-golden/laa-non-standard-crime-claims-golden-signals?orgId=1&refresh=5m&var-datasource=prometheus&var-namespace=laa-submit-crime-forms-dev

    - alert: Nginx5xxError
      expr: |-
        # Any 500|502|503|504 statuses in previous 2 minutes
        # 
        # Note that this approach, while verbose, is more sensitive than detecting `rate` or `increase` on the nginx_ingress_controller_requests counter.
        # Start with effectively a dictionary of ingress names with the current value of the errors-since-records-began counter (C) for each
        # 
        # Now subtract the value of the relevant counter 2 minutes ago, so can see how the counter has changed in the last 2 minutes
        # If for ingress X, C has increased from 3 to 5 in the last 2 minutes, this subtraction will do {X: 5} - {X: 3} and will result in {X: 2}.
        # 
        # But here's the twist: The counter doesn't start at zero, it starts as undefined. If the counter was undefined 2 minutes ago, but now 
        # has a value of 1, by itself the subtraction becomes {X: 1} - {}, and thanks to how PromQL handles vectors, this will result in {}.
        # So we need to make sure that in this scenario, the second dictionary has a value for X that is less than the current value of C for X.
        # To achieve this we use "or" to fall back to a value of C - 1, so that our subtraction becomes {X: 1} - {X: 0} = {X: 1}
        # 
        # Finally, for the purposes of this alert, we filter out any values of our resulting dictionary that are not positive.
            sum by (ingress) (
              nginx_ingress_controller_requests{exported_namespace="laa-submit-crime-forms-dev",status=~"500|502|503|504"}
            )
          -
            (
                sum by (ingress) (
                  nginx_ingress_controller_requests{exported_namespace="laa-submit-crime-forms-dev",status=~"500|502|503|504"} offset 2m
                )
              or
                (
                    sum by (ingress) (
                      nginx_ingress_controller_requests{exported_namespace="laa-submit-crime-forms-dev",status=~"500|502|503|504"}
                    )
                  -
                    1
                )
            )
        >
          0
      labels:
        severity: laa-crime-forms-team-pre-prod
      annotations:
        message: Namespace laa-submit-crime-forms-dev encountered one or more HTTP 5xx errors
        runbook_url: https://dsdmoj.atlassian.net/wiki/spaces/CRM457/pages/4634181917/Runbooks
        dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/laa-crime-store-grafana-golden/laa-non-standard-crime-claims-golden-signals?orgId=1&refresh=5m&var-datasource=prometheus&var-namespace=laa-submit-crime-forms-dev

    - alert: LowPodCount
      expr: sum (kube_pod_status_phase{namespace="laa-submit-crime-forms-dev", phase="Running"}) < 1
      labels:
        severity: laa-crime-forms-team-pre-prod
      annotations:
        message: Namespace laa-submit-crime-forms-dev has fewer than expected number of running pods
        runbook_url: https://dsdmoj.atlassian.net/wiki/spaces/CRM457/pages/4634181917/Runbooks
        dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/laa-crime-store-grafana-golden/laa-non-standard-crime-claims-golden-signals?orgId=1&refresh=5m&var-datasource=prometheus&var-namespace=laa-submit-crime-forms-dev

    - alert: KubeQuotaExceeded
      expr: 100 * kube_resourcequota{job="kube-state-metrics",type="used",namespace="laa-submit-crime-forms-dev"} / ignoring(instance, job, type) (kube_resourcequota{job="kube-state-metrics",type="hard",namespace="laa-submit-crime-forms-dev"} > 0) > 90
      for: 1m
      labels:
        severity: laa-crime-forms-team-pre-prod
      annotations:
        message: Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value}}% of its {{ $labels.resource }} quota.
        runbook_url: https://dsdmoj.atlassian.net/wiki/spaces/CRM457/pages/4634181917/Runbooks
        dashboard_url: https://grafana.live.cloud-platform.service.justice.gov.uk/d/laa-crime-store-grafana-golden/laa-non-standard-crime-claims-golden-signals?orgId=1&refresh=5m&var-datasource=prometheus&var-namespace=laa-submit-crime-forms-dev  

    - alert: KubePodCrashLooping
      annotations:
        message: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
        runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping
      expr: |-
        rate(kube_pod_container_status_restarts_total{job="kube-state-metrics", namespace="laa-submit-crime-forms-dev"}[15m]) * 60 * 5 > 0
      for: 1h
      labels:
        severity: laa-crime-forms-team-pre-prod